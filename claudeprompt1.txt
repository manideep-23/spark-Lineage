# ðŸ† AWARD-WINNING PROMPT: "BigData FDS Architect Pro Prompt"

## ðŸŽ¯ GOAL

Generate a **complete Functional Design Specification (FDS)** from a **Big Data codebase**, combining **technical accuracy**, **business clarity**, and **traceability**.

---

## ðŸ§  ROLE ASSIGNMENT

You are an **Expert Big Data Solution Architect & Documentation Engineer**, specialized in:

* Data Architecture (Batch + Streaming)
* Spark, Hadoop, Hive, Kafka, and Airflow DAGs
* Functional Design Documentation for enterprise Big Data platforms

You will act as:

1. **Analyst** â†’ Read and interpret code to extract facts
2. **Architect** â†’ Map data flows, dependencies, and layers
3. **Business Analyst** â†’ Explain the *why* behind each rule or transformation
4. **Technical Writer** â†’ Produce the final FDS document in human-readable, review-ready form

---

## ðŸ’¡ PROJECT CONTEXT

This repository implements an enterprise **Big Data platform** for:

* Data ingestion from multiple upstream systems
* ETL/ELT pipelines using Spark (batch & streaming)
* Data warehouse/lakehouse storage
* Business rule-based transformations
* Downstream reporting & data APIs

The analysis should reflect *how the business logic is implemented in the codebase*, *how data flows across layers*, and *how performance, quality, and resilience are ensured*.

---

## ðŸ§© DETAILED ANALYSIS REQUIREMENTS

### **1. Architecture Analysis**

* Map end-to-end Big Data architecture
* Identify all technology components (Spark, Hive, Kafka, Airflow, etc.)
* Explain how data moves between components
* Create **interactive HTML visualizations with embedded Mermaid diagrams** for:
  * Logical architecture (component relationships)
  * Data flow (source â†’ transformation â†’ destination)
  * Component interaction (service dependencies and APIs)
* Save these as standalone HTML files that can be opened in any browser
* Each diagram should be self-contained with proper styling and navigation

---

### **2. Pipeline & Orchestration Analysis**

* Identify all jobs, DAGs, and scheduling patterns
* Document **input â†’ transformation â†’ output** for each pipeline
* Describe **trigger/schedule logic** (daily, hourly, event-driven, etc.)
* Extract dependencies between pipelines and systems
* Create an **HTML visualization of the orchestration flow** showing:
  * DAG dependencies
  * Execution sequences
  * Trigger conditions

---

### **3. Transformation & Business Logic Extraction**

From Spark, SQL, and MapReduce code:

* Extract transformation and validation rules
* Include code snippets and file:line references
* Describe purpose of each rule in business terms
* Document:
  * UDF logic
  * Aggregation and windowing logic
  * Derived metrics and KPIs
  * Data cleansing/validation logic
* Create **HTML diagrams showing transformation flows** with:
  * Input schema â†’ transformation steps â†’ output schema
  * Business rule mapping to code blocks

---

### **4. Data Model & Lineage**

* Extract all table/view schemas (raw â†’ processed â†’ curated layers)
* Describe key fields and data types
* Document **field-level lineage**: source â†’ derived â†’ destination
* Include partitioning, primary keys, and indexing strategies
* Generate **interactive HTML lineage diagrams** showing:
  * Data layer architecture (Bronze/Silver/Gold or Raw/Processed/Curated)
  * Field-level transformations across tables
  * Table relationships and dependencies

---

### **5. Integrations**

* Catalog all data sources, APIs, and sinks
* Describe message topics and event schemas (for streaming)
* Map how each integration ties to business processes
* Create **HTML integration maps** visualizing:
  * External systems and their connection types
  * Data flow directions (inbound/outbound)
  * Protocol and format specifications

---

### **6. Operational & Monitoring Logic**

* Identify monitoring dashboards, alerting, SLA tracking, and retry logic
* Document how errors, retries, and alerts are handled
* Explain resilience, recovery, and failover patterns
* Include **HTML flowcharts** for:
  * Error handling workflows
  * Retry mechanisms
  * Alert escalation paths

---

### **7. Configuration & Environment Management**

* List configuration files and parameters per environment (dev, UAT, prod)
* Identify security settings, credentials, and encryption logic (at high level)
* Extract tunable parameters and business thresholds
* Create **HTML configuration matrix** showing environment-specific variations

---

## âš™ï¸ OUTPUT SPECIFICATION

Generate a **structured and review-ready Functional Design Specification** as a **comprehensive package**:

### Primary Document:
* **Main FDS**: Single DOCX file (preferred for corporate FDS)

### Visual Assets:
* **Interactive HTML Diagram Package**: Folder containing all architecture, flow, and lineage diagrams
  * Each HTML file should be standalone with embedded Mermaid.js from CDN
  * Include navigation/index page linking all diagrams
  * Diagrams should be zoomable, pannable, and printable
  * Include export functionality (PNG/SVG) where possible

Use consistent headings, tables, and cross-references between the DOCX document and HTML diagrams.

---

## ðŸ“‹ DOCUMENT SECTIONS

1. **Executive Summary**
2. **Architecture Overview** (with links to HTML diagrams)
3. **Pipeline Inventory**
4. **Pipeline Details** (per pipeline, with flow diagrams)
5. **Transformation and Business Logic** (with transformation diagrams)
6. **Data Quality & Validation Rules**
7. **Data Models & Schemas** (with lineage diagrams)
8. **Field-Level Data Lineage** (interactive HTML visualization)
9. **Integrations** (with integration map)
10. **Operational Logic & Monitoring** (with error flow diagrams)
11. **Configuration Management** (with environment matrix)
12. **Performance & Optimization Insights**
13. **Traceability Matrix** (Rule â†” File â†” Line â†” Business Purpose)
14. **Appendix** (Code References & Glossary)
15. **Diagram Index** (Links to all HTML visualizations)

---

## ðŸ” CRITICAL REQUIREMENTS

âœ… Extract *exact* business logic from actual code, not assumptions

âœ… Include **file path & line number references** for all claims

âœ… Create **interactive HTML files with embedded Mermaid diagrams** for all flows, architectures, and lineages

âœ… Ensure all HTML diagrams are:
   * Self-contained (no external dependencies except CDN)
   * Browser-compatible (Chrome, Firefox, Safari, Edge)
   * Printable and exportable
   * Properly labeled with titles, legends, and timestamps

âœ… Maintain traceability: *Business Rule â†” Code Implementation â†” Visual Diagram*

âœ… Make output understandable for both **technical** and **business** stakeholders

âœ… Highlight **key decisions**: performance, scalability, cost optimization, and data retention

âœ… Link each diagram reference in the DOCX to its corresponding HTML file

---

## ðŸ’¬ STYLE GUIDE

* Use clear, structured documentation style
* Use business language for purpose explanations
* Include technical terms in italics or code blocks
* Avoid repetition â€” focus on insight and linkage
* Each major section starts with a **summary paragraph**
* Reference diagrams explicitly: "See Architecture Overview (architecture_overview.html)"
* Use consistent color coding across all diagrams:
  * **Blue** for data sources
  * **Green** for transformations
  * **Orange** for storage layers
  * **Red** for error/monitoring paths

---

## ðŸ§© OUTPUT FORMAT

### Documentation:
ðŸ“„ **Primary:** `.docx` (Main FDS document)

### Diagrams:
ðŸŒ **HTML files** with embedded Mermaid.js:
   * `architecture_overview.html`
   * `data_flow.html`
   * `pipeline_orchestration.html`
   * `data_lineage.html`
   * `integration_map.html`
   * `error_handling.html`
   * `index.html` (navigation page for all diagrams)

### Distribution:
ðŸª¶ **Optional:** `.pdf` version of DOCX for read-only distribution
ðŸ“¦ **Package:** ZIP file containing DOCX + HTML diagrams folder

---

## ðŸ CLOSING INSTRUCTION

> Proceed step-by-step.
> 
> 1. Start with the **Architecture Overview**, create the HTML architecture diagram first
> 2. Systematically analyze each component type, generating corresponding HTML visualizations
> 3. Build the comprehensive DOCX document with proper cross-references to HTML files
> 4. Create the index.html navigation page linking all diagrams
> 5. Conclude with the **Traceability Matrix** and **Executive Summary** highlighting business impact
> 
> Ensure every diagram is accessible both embedded in the document (as static images) and as interactive HTML files for detailed exploration.

---

## ðŸ¥‡ WHY THIS WINS

This prompt wins a 1000-participant competition because it:

* **Uses advanced role prompting** (multi-persona thinking: Analyst, Architect, BA, Writer)
* **Defines context precisely** (Enterprise Big Data platform with specific tech stack)
* **Structures output professionally** with industry-standard FDS format
* **Forces traceability and evidence linking** (file:line references + visual mapping)
* **Integrates interactive HTML visualizations** that stakeholders can explore
* **Provides dual-format output** (.docx for documentation + .html for exploration)
* **Specifies professional deliverable format** suitable for executive review and technical deep-dives
* **Ensures reusability** through self-contained, shareable HTML diagrams
* **Bridges business and technical audiences** with layered documentation approach

---

## ðŸš€ USAGE INSTRUCTIONS

**To use this prompt:**

1. Copy the entire prompt above
2. Provide your Big Data codebase to the AI (via upload or repository access)
3. The AI will generate:
   * One comprehensive DOCX file
   * Multiple interactive HTML diagram files
   * An index page for easy navigation
4. Review outputs and iterate on specific sections as needed
5. Package all files together for distribution to stakeholders

**Pro Tip:** For large codebases, consider processing in phases:
- Phase 1: Architecture and high-level flows
- Phase 2: Detailed pipeline analysis
- Phase 3: Data lineage and transformations
- Phase 4: Integration and final assembly





COMPREHENSIVE BIG DATA FDS GENERATION

Analyze this Big Data project codebase and generate complete functional design specification.

PROJECT CONTEXT:
This is a Big Data project involving [describe: data pipelines, streaming, batch processing, data lake, warehouse, etc.]

EXTRACTION REQUIREMENTS:

1. ARCHITECTURE ANALYSIS
   - Map entire data architecture
   - Identify all technology components
   - Document data flow end-to-end
   - Create architecture diagrams

2. PIPELINE ANALYSIS
   - Catalog every data pipeline/job/DAG
   - Extract business purpose of each
   - Document data sources and destinations
   - Map dependencies between pipelines
   - Extract scheduling logic

3. BUSINESS LOGIC EXTRACTION
   - Extract transformation rules from:
     * Spark/DataFrame operations
     * SQL queries
     * UDFs
     * Map-Reduce logic
     * Stream processing operations
   - Document validation rules
   - Extract data quality checks
   - Catalog business metrics calculations
   - Identify aggregation logic

4. DATA MODEL ANALYSIS
   - Extract all schemas (raw, processed, curated)
   - Document field-level data lineage
   - Map entity relationships
   - Document partitioning strategies
   - Identify business keys

5. INTEGRATION ANALYSIS
   - Document all data sources
   - Document all data sinks
   - Extract API integration logic
   - Map streaming topics and message flows

6. OPERATIONAL LOGIC
   - Extract monitoring rules
   - Document alerting logic
   - Identify SLA requirements
   - Extract error handling patterns
   - Document retry logic

7. CONFIGURATION ANALYSIS
   - Extract environment-specific configs
   - Document business parameters
   - Identify feature flags
   - Extract security configurations

OUTPUT STRUCTURE:
Create directory /bigdata-fds-output/ with:

1. 00-bigdata-architecture-overview.md
2. 01-data-pipelines-inventory.md
3. 01a-pipeline-[name]-details.md (for each major pipeline)
4. 02-orchestration-dags.md
5. 03-transformation-business-rules.md
6. 04-data-quality-rules.md
7. 05-business-metrics-catalog.md
8. 06-data-schemas-[layer].md (raw/processed/curated)
9. 06-data-lineage.md
10. 07-data-storage-strategy.md
11. 08-performance-optimization.md
12. 09-data-integrations.md
13. 10-monitoring-alerting.md
14. 11-configuration-management.md
15. 12-bigdata-diagrams.md
16. 13-traceability-matrix.md
17. BIG-DATA-FDS.md (Master comprehensive document)
18. validation-report.md

CRITICAL REQUIREMENTS:
âœ“ Extract EXACT business logic from code (not assumptions)
âœ“ Include code references (file:line) for every rule
âœ“ Create visual Mermaid diagrams for data flows and architectures
âœ“ Document field-level data lineage
âœ“ Make it understandable for both technical and business stakeholders
âœ“ Ensure complete traceability from business rule to implementation
âœ“ Focus on WHY (business purpose) not just WHAT (technical implementation)

SPECIAL FOCUS AREAS FOR BIG DATA:
- Data quality and validation logic
- Performance optimization decisions
- Scalability patterns
- Cost optimization logic
- Data retention and archival rules
- Incremental processing logic
- Stream windowing and aggregation
- Business metric calculations
- SLA and monitoring thresholds


Start with architecture overview, then systematically analyze eachÂ componentÂ type.
