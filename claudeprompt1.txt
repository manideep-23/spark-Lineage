# 🏆 AWARD-WINNING PROMPT: "BigData FDS Architect Pro Prompt"

## 🎯 GOAL

Generate a **complete Functional Design Specification (FDS)** from a **Big Data codebase**, combining **technical accuracy**, **business clarity**, and **traceability**.

---

## 🧠 ROLE ASSIGNMENT

You are an **Expert Big Data Solution Architect & Documentation Engineer**, specialized in:

* Data Architecture (Batch + Streaming)
* Spark, Hadoop, Hive, Kafka, and Airflow DAGs
* Functional Design Documentation for enterprise Big Data platforms

You will act as:

1. **Analyst** → Read and interpret code to extract facts
2. **Architect** → Map data flows, dependencies, and layers
3. **Business Analyst** → Explain the *why* behind each rule or transformation
4. **Technical Writer** → Produce the final FDS document in human-readable, review-ready form

---

## 💡 PROJECT CONTEXT

This repository implements an enterprise **Big Data platform** for:

* Data ingestion from multiple upstream systems
* ETL/ELT pipelines using Spark (batch & streaming)
* Data warehouse/lakehouse storage
* Business rule-based transformations
* Downstream reporting & data APIs

The analysis should reflect *how the business logic is implemented in the codebase*, *how data flows across layers*, and *how performance, quality, and resilience are ensured*.

---

## 🧩 DETAILED ANALYSIS REQUIREMENTS

### **1. Architecture Analysis**

* Map end-to-end Big Data architecture
* Identify all technology components (Spark, Hive, Kafka, Airflow, etc.)
* Explain how data moves between components
* Create **interactive HTML visualizations with embedded Mermaid diagrams** for:
  * Logical architecture (component relationships)
  * Data flow (source → transformation → destination)
  * Component interaction (service dependencies and APIs)
* Save these as standalone HTML files that can be opened in any browser
* Each diagram should be self-contained with proper styling and navigation

---

### **2. Pipeline & Orchestration Analysis**

* Identify all jobs, DAGs, and scheduling patterns
* Document **input → transformation → output** for each pipeline
* Describe **trigger/schedule logic** (daily, hourly, event-driven, etc.)
* Extract dependencies between pipelines and systems
* Create an **HTML visualization of the orchestration flow** showing:
  * DAG dependencies
  * Execution sequences
  * Trigger conditions

---

### **3. Transformation & Business Logic Extraction**

From Spark, SQL, and MapReduce code:

* Extract transformation and validation rules
* Include code snippets and file:line references
* Describe purpose of each rule in business terms
* Document:
  * UDF logic
  * Aggregation and windowing logic
  * Derived metrics and KPIs
  * Data cleansing/validation logic
* Create **HTML diagrams showing transformation flows** with:
  * Input schema → transformation steps → output schema
  * Business rule mapping to code blocks

---

### **4. Data Model & Lineage**

* Extract all table/view schemas (raw → processed → curated layers)
* Describe key fields and data types
* Document **field-level lineage**: source → derived → destination
* Include partitioning, primary keys, and indexing strategies
* Generate **interactive HTML lineage diagrams** showing:
  * Data layer architecture (Bronze/Silver/Gold or Raw/Processed/Curated)
  * Field-level transformations across tables
  * Table relationships and dependencies

---

### **5. Integrations**

* Catalog all data sources, APIs, and sinks
* Describe message topics and event schemas (for streaming)
* Map how each integration ties to business processes
* Create **HTML integration maps** visualizing:
  * External systems and their connection types
  * Data flow directions (inbound/outbound)
  * Protocol and format specifications

---

### **6. Operational & Monitoring Logic**

* Identify monitoring dashboards, alerting, SLA tracking, and retry logic
* Document how errors, retries, and alerts are handled
* Explain resilience, recovery, and failover patterns
* Include **HTML flowcharts** for:
  * Error handling workflows
  * Retry mechanisms
  * Alert escalation paths

---

### **7. Configuration & Environment Management**

* List configuration files and parameters per environment (dev, UAT, prod)
* Identify security settings, credentials, and encryption logic (at high level)
* Extract tunable parameters and business thresholds
* Create **HTML configuration matrix** showing environment-specific variations

---

## ⚙️ OUTPUT SPECIFICATION

Generate a **structured and review-ready Functional Design Specification** as a **comprehensive package**:

### Primary Document:
* **Main FDS**: Single DOCX file (preferred for corporate FDS)

### Visual Assets:
* **Interactive HTML Diagram Package**: Folder containing all architecture, flow, and lineage diagrams
  * Each HTML file should be standalone with embedded Mermaid.js from CDN
  * Include navigation/index page linking all diagrams
  * Diagrams should be zoomable, pannable, and printable
  * Include export functionality (PNG/SVG) where possible

Use consistent headings, tables, and cross-references between the DOCX document and HTML diagrams.

---

## 📋 DOCUMENT SECTIONS

1. **Executive Summary**
2. **Architecture Overview** (with links to HTML diagrams)
3. **Pipeline Inventory**
4. **Pipeline Details** (per pipeline, with flow diagrams)
5. **Transformation and Business Logic** (with transformation diagrams)
6. **Data Quality & Validation Rules**
7. **Data Models & Schemas** (with lineage diagrams)
8. **Field-Level Data Lineage** (interactive HTML visualization)
9. **Integrations** (with integration map)
10. **Operational Logic & Monitoring** (with error flow diagrams)
11. **Configuration Management** (with environment matrix)
12. **Performance & Optimization Insights**
13. **Traceability Matrix** (Rule ↔ File ↔ Line ↔ Business Purpose)
14. **Appendix** (Code References & Glossary)
15. **Diagram Index** (Links to all HTML visualizations)

---

## 🔍 CRITICAL REQUIREMENTS

✅ Extract *exact* business logic from actual code, not assumptions

✅ Include **file path & line number references** for all claims

✅ Create **interactive HTML files with embedded Mermaid diagrams** for all flows, architectures, and lineages

✅ Ensure all HTML diagrams are:
   * Self-contained (no external dependencies except CDN)
   * Browser-compatible (Chrome, Firefox, Safari, Edge)
   * Printable and exportable
   * Properly labeled with titles, legends, and timestamps

✅ Maintain traceability: *Business Rule ↔ Code Implementation ↔ Visual Diagram*

✅ Make output understandable for both **technical** and **business** stakeholders

✅ Highlight **key decisions**: performance, scalability, cost optimization, and data retention

✅ Link each diagram reference in the DOCX to its corresponding HTML file

---

## 💬 STYLE GUIDE

* Use clear, structured documentation style
* Use business language for purpose explanations
* Include technical terms in italics or code blocks
* Avoid repetition — focus on insight and linkage
* Each major section starts with a **summary paragraph**
* Reference diagrams explicitly: "See Architecture Overview (architecture_overview.html)"
* Use consistent color coding across all diagrams:
  * **Blue** for data sources
  * **Green** for transformations
  * **Orange** for storage layers
  * **Red** for error/monitoring paths

---

## 🧩 OUTPUT FORMAT

### Documentation:
📄 **Primary:** `.docx` (Main FDS document)

### Diagrams:
🌐 **HTML files** with embedded Mermaid.js:
   * `architecture_overview.html`
   * `data_flow.html`
   * `pipeline_orchestration.html`
   * `data_lineage.html`
   * `integration_map.html`
   * `error_handling.html`
   * `index.html` (navigation page for all diagrams)

### Distribution:
🪶 **Optional:** `.pdf` version of DOCX for read-only distribution
📦 **Package:** ZIP file containing DOCX + HTML diagrams folder

---

## 🏁 CLOSING INSTRUCTION

> Proceed step-by-step.
> 
> 1. Start with the **Architecture Overview**, create the HTML architecture diagram first
> 2. Systematically analyze each component type, generating corresponding HTML visualizations
> 3. Build the comprehensive DOCX document with proper cross-references to HTML files
> 4. Create the index.html navigation page linking all diagrams
> 5. Conclude with the **Traceability Matrix** and **Executive Summary** highlighting business impact
> 
> Ensure every diagram is accessible both embedded in the document (as static images) and as interactive HTML files for detailed exploration.

---

## 🥇 WHY THIS WINS

This prompt wins a 1000-participant competition because it:

* **Uses advanced role prompting** (multi-persona thinking: Analyst, Architect, BA, Writer)
* **Defines context precisely** (Enterprise Big Data platform with specific tech stack)
* **Structures output professionally** with industry-standard FDS format
* **Forces traceability and evidence linking** (file:line references + visual mapping)
* **Integrates interactive HTML visualizations** that stakeholders can explore
* **Provides dual-format output** (.docx for documentation + .html for exploration)
* **Specifies professional deliverable format** suitable for executive review and technical deep-dives
* **Ensures reusability** through self-contained, shareable HTML diagrams
* **Bridges business and technical audiences** with layered documentation approach

---

## 🚀 USAGE INSTRUCTIONS

**To use this prompt:**

1. Copy the entire prompt above
2. Provide your Big Data codebase to the AI (via upload or repository access)
3. The AI will generate:
   * One comprehensive DOCX file
   * Multiple interactive HTML diagram files
   * An index page for easy navigation
4. Review outputs and iterate on specific sections as needed
5. Package all files together for distribution to stakeholders

**Pro Tip:** For large codebases, consider processing in phases:
- Phase 1: Architecture and high-level flows
- Phase 2: Detailed pipeline analysis
- Phase 3: Data lineage and transformations
- Phase 4: Integration and final assembly





COMPREHENSIVE BIG DATA FDS GENERATION

Analyze this Big Data project codebase and generate complete functional design specification.

PROJECT CONTEXT:
This is a Big Data project involving [describe: data pipelines, streaming, batch processing, data lake, warehouse, etc.]

EXTRACTION REQUIREMENTS:

1. ARCHITECTURE ANALYSIS
   - Map entire data architecture
   - Identify all technology components
   - Document data flow end-to-end
   - Create architecture diagrams

2. PIPELINE ANALYSIS
   - Catalog every data pipeline/job/DAG
   - Extract business purpose of each
   - Document data sources and destinations
   - Map dependencies between pipelines
   - Extract scheduling logic

3. BUSINESS LOGIC EXTRACTION
   - Extract transformation rules from:
     * Spark/DataFrame operations
     * SQL queries
     * UDFs
     * Map-Reduce logic
     * Stream processing operations
   - Document validation rules
   - Extract data quality checks
   - Catalog business metrics calculations
   - Identify aggregation logic

4. DATA MODEL ANALYSIS
   - Extract all schemas (raw, processed, curated)
   - Document field-level data lineage
   - Map entity relationships
   - Document partitioning strategies
   - Identify business keys

5. INTEGRATION ANALYSIS
   - Document all data sources
   - Document all data sinks
   - Extract API integration logic
   - Map streaming topics and message flows

6. OPERATIONAL LOGIC
   - Extract monitoring rules
   - Document alerting logic
   - Identify SLA requirements
   - Extract error handling patterns
   - Document retry logic

7. CONFIGURATION ANALYSIS
   - Extract environment-specific configs
   - Document business parameters
   - Identify feature flags
   - Extract security configurations

OUTPUT STRUCTURE:
Create directory /bigdata-fds-output/ with:

1. 00-bigdata-architecture-overview.md
2. 01-data-pipelines-inventory.md
3. 01a-pipeline-[name]-details.md (for each major pipeline)
4. 02-orchestration-dags.md
5. 03-transformation-business-rules.md
6. 04-data-quality-rules.md
7. 05-business-metrics-catalog.md
8. 06-data-schemas-[layer].md (raw/processed/curated)
9. 06-data-lineage.md
10. 07-data-storage-strategy.md
11. 08-performance-optimization.md
12. 09-data-integrations.md
13. 10-monitoring-alerting.md
14. 11-configuration-management.md
15. 12-bigdata-diagrams.md
16. 13-traceability-matrix.md
17. BIG-DATA-FDS.md (Master comprehensive document)
18. validation-report.md

CRITICAL REQUIREMENTS:
✓ Extract EXACT business logic from code (not assumptions)
✓ Include code references (file:line) for every rule
✓ Create visual Mermaid diagrams for data flows and architectures
✓ Document field-level data lineage
✓ Make it understandable for both technical and business stakeholders
✓ Ensure complete traceability from business rule to implementation
✓ Focus on WHY (business purpose) not just WHAT (technical implementation)

SPECIAL FOCUS AREAS FOR BIG DATA:
- Data quality and validation logic
- Performance optimization decisions
- Scalability patterns
- Cost optimization logic
- Data retention and archival rules
- Incremental processing logic
- Stream windowing and aggregation
- Business metric calculations
- SLA and monitoring thresholds


Start with architecture overview, then systematically analyze each component type.
