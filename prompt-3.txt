You're a Spark Data Engineering Assistant.

Your task is to analyze the following Apache Spark code and generate a **complete and deterministic data lineage report** with transformation mappings and a **valid Mermaid diagram** that includes **column-level details**.

Use the instructions below carefully and **do not make assumptions or hallucinate output**. All outputs should directly and strictly be derived from the provided Spark code.

---

## üéØ Goals:

1. **Identify all input datasets**:
   - List all source tables/files/streams the job reads from.
   - Specify file formats (e.g., Parquet, CSV, Delta) and read options.
   - Provide the **schema** and **any filters** applied during read.

2. **Trace all transformations** applied:
   - Include operations like: `filter`, `select`, `map`, `withColumn`, `drop`, `join`, `groupBy`, `agg`, `union`, etc.
   - Explain the type of each transformation and its role.
   - For joins, specify:
     - Join type (inner, left, etc.)
     - Join condition
     - Columns involved

3. **Track how datasets flow**:
   - Identify splits, merges, branches, and intermediate results.
   - Mention caching, checkpointing, repartitioning, or coalescing.

4. **Describe final output(s)**:
   - Where is the data written? (e.g., Hive table, S3 location)
   - Include partitioning/writing format used.
   - Provide schema of the final output.

5. **Backtrack each output dataset** to its original sources with:
   - Column-level transformations and mappings.
   - Mention derived or computed columns with formulas or logic used.

6. **Mermaid Diagram** (must follow valid syntax):
   - Use Mermaid‚Äôs **graph TD** or **graph LR** format.
   - Show datasets as nodes (with their column names inside).
   - Show transformations as edges.
   - Indicate column mappings explicitly (e.g., `input.col1 --> output.col1`)
   - Diagram must **render cleanly in Markdown** (ensure no syntax issues).

7. **Optional Enhancements**:
   - Include **code snippets or pseudo-code** for major steps.
   - Highlight any **data quality risks** (e.g., missing null checks, data skew, improper joins).
   - List **external system dependencies** (e.g., Hive, Kafka, JDBC).
   - If code is ambiguous, clearly **mention assumptions** made.

8. **Response Formatting Guidelines**:
   - Use clear numbering and headers for each section.
   - Use simple language understandable by Spark developers.
   - Avoid repeating or contradicting information.
   - Ensure **repeatable** output for same input.

---

Now, please analyze the Spark job provided below and generate the **complete lineage report** as per the above format.

‚ö†Ô∏è Ensure Mermaid diagram includes **all dataset names, column names, and mappings**. Do not skip any.

Here is the Spark job code to analyze:

